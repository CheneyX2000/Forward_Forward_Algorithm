import numpy as np

class FFLayer:
    def __init__(self, input_dim, output_dim, learning_rate=0.1):
        self.in_dim = input_dim
        self.out_dim = output_dim
        self.W = np.random.randn(self.in_dim, self.out_dim)
        self.b = np.zeros((self.out_dim,))  # bias init to zero
        self.eta = learning_rate

    def forward(self, x) -> np.array:
        # x is np array that x.shape = (input_dim,)
        # Weight is np array that W.shape = (input_dim, output_dim)
        # so the activation a is np array that a.shape = (output_dim,)
        # use tanh as default activation func
        a = np.tanh(np.dot(x, self.W) + self.b)
        return a

    def compute_goodness(self, activations) -> float:
        return np.sum(activations ** 2)

    def update_weights(self, x, activations, is_positive) -> None:
        # the diff of tanh is 1 - tanh ** 2
        tanh_prime = 1 - activations ** 2
        # compute the grad for W and b
        grad_W = 2 * np.outer(x, activations * tanh_prime)
        grad_b = 2 * activations * tanh_prime

        if is_positive:
            self.W += self.eta * grad_W
            self.b += self.eta * grad_b
        else:
            self.W -= self.eta * grad_W
            self.b -= self.eta * grad_b