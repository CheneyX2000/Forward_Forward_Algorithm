2025.09.09

Have trained with specs:
eta = [0.01, 0.004],
and layers in shapes:
[794, 200, 10],          # shallow
[794, 500, 300, 10],     # medium
[794, 800, 400, 200, 10] # deep
have not get ideal accuracy (overall accuracy [60, 74]).

Decide to use smaller learning rate: 0.0001, 
and use wider hidden layers:
794 -> 2000 -> 2000 -> 2000 -> 2000 -> 10
as written in Hinton's paper.

Next step:
New training script with bigger data set (full MNIST), wider hidden layers (4 x 2000), smaller learning rate (0.0001).
Try ReLu as the activation func.
Probably need to refactor the impl in order to use GPU and save training time.
